Lesson Outline
This lesson will cover training and tuning, including each of the following topics.

Types of errors
Model complexity graph
Cross validation
Learning curves
Grid search
Grid search in sklearn
Putting it all together
At a high level, all machine learning follows the same flow, as shown below. We have talked about several models and metrics, and in this lesson we will talk about comparing models to ensure the best predictive results.

Machine learning process of training, cross validating, and testing
Machine learning process of training, cross validating, and testing

Learning Objectives
By the end of the Training and Tuning lesson, you should be able to

Train and test models with Scikit-learn
Choose the best model using evaluation techniques like cross-validation and grid search
Where are you in the course?
Machine Learning Brid's Eye View
Linear Regression
Perceptron Algorithm
Decision Trees
Naive Bayes
Support Vector Machines
Ensemble Methods
Model Evaluation Metrics
Training and Tuning
Project

-----

Lesson Review
Congratulations. Now you have to learn how to train models, how to test them, how to use certain metrics to evaluate how good these models are, and, finally, how to tune the parameters to improve your model.

Specifically, we discussed each of the following topics:

Types of errors
Model complexity graph
Cross-validation
Learning curves
Grid search
Grid search in sklearn
Putting it all together

Glossary
Key Term	Definition
Bias	The bias is known as the difference between the prediction of the values by the ML model and the correct value. High bias reslts in a large error in training as well as testing data.
Grid search	A table with results of all probabilities and the best result is used.
K-fold cross validation	A parameter called 'k' that represents the number of groups that a given data sample is to be split into. Then the results for all subsets are averaged.
Learning curves	A tool used in machine learning that learns from a training dataset incrementally, and the plot shows changes in learning performance over time.
Overfitting	When a model uses a lines which is too complex, as if memorizing the training set, and won't generalize well. Over-complication of the problem
Underfitting	When a model uses a line, which is too simplistic. It doesn't do very well on the training set. Oversimplification of the problem
Variance	iA type of error due to a model's sensitivity to small fluctuations in the training set. High variance would cause an algorithm to model the noise in the training set. This is known as overfitting
